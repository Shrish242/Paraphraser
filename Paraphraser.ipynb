{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b5001e-a7db-43bf-bc1d-ca704d25245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f11895b-da31-415b-baa9-b3df4d199ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549a869-dd86-4423-89c4-4e6af1749d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b1cefc-47cf-4ebd-a8ed-67710e320654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#testing torch\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())  # Check if CUDA is available (if you have a GPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1895c989-bc19-406a-a66f-86b167dc13ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I love using transformers!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947dc762-cddd-4bea-bd11-b770b4d6f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing transformers second time\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the sentiment-analysis pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Example texts\n",
    "texts = [\n",
    "    \"I love studying Computer Science!\",\n",
    "    \"I am not happy with the current situation.\",\n",
    "    \"The movie was fantastic!\",\n",
    "    \"I'm feeling a bit sad today.\"\n",
    "]\n",
    "\n",
    "# Analyze the sentiment of each text\n",
    "for text in texts:\n",
    "    result = classifier(text)[0]\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {result['label']}, Score: {result['score']:.4f}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f3a5fb-d2f8-4898-8e83-b8b09e44c801",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying paraphase with pretrained model from transformers\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "# Load the Pegasus model and tokenizer fine-tuned for paraphrasing\n",
    "model_name = \"tuner007/pegasus_paraphrase\"\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def paraphrase(text, max_length=50, num_beams=10, temperature=1.5):\n",
    "    # Tokenize and prepare the input text\n",
    "    input_text = tokenizer(text, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate paraphrased text using the model\n",
    "    outputs = model.generate(\n",
    "        **input_text,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        temperature=temperature,\n",
    "        early_stopping=True,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    # Decode the output to text\n",
    "    paraphrased_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return paraphrased_text\n",
    "\n",
    "# Main loop to ask for user input and provide paraphrases\n",
    "while True:\n",
    "    # Ask the user for a sentence to paraphrase\n",
    "    user_input = input(\"Enter a sentence to paraphrase (or type 'exit' to quit): \")\n",
    "    \n",
    "    # Exit condition\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    \n",
    "    # Paraphrase the user input\n",
    "    paraphrased_output = paraphrase(user_input, max_length=60, num_beams=10, temperature=1.5)\n",
    "    \n",
    "    # Display the original and paraphrased sentences\n",
    "    print(f\"Original: {user_input}\")\n",
    "    print(f\"Paraphrased: {paraphrased_output}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaff704b-d96d-4081-9593-9335b389c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a5edc8-0cb2-4e87-b0ec-1ac342f4adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7de15a-e032-44d4-ab6c-9b2e1a7ac5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paraphasing and training the model from the csv file\n",
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load and prepare the dataset\n",
    "df = pd.read_csv(\"Dataset.csv\")\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Tokenize the dataset\n",
    "model_name = \"tuner007/pegasus_paraphrase\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    input_texts = tokenizer(examples['original_sentence'], truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "    target_texts = tokenizer(examples['paraphrased_sentence'], truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "    \n",
    "    # Return input_ids and attention_mask for inputs, and input_ids as labels for targets\n",
    "    return {\n",
    "        'input_ids': input_texts['input_ids'].squeeze(),\n",
    "        'attention_mask': input_texts['attention_mask'].squeeze(),\n",
    "        'labels': target_texts['input_ids'].squeeze()\n",
    "    }\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",  # Use \"epoch\" for both strategies\n",
    "    save_strategy=\"epoch\",  # Match this with eval_strategy\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\"\n",
    ")\n",
    "\n",
    "#Initialize the Trainer\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model with safe serialization disabled\n",
    "model.save_pretrained(\"./fine_tuned_pegasus_v2\", safe_serialization=False)\n",
    "tokenizer.save_pretrained(\"./fine_tuned_pegasus_v2\")\n",
    "\n",
    "# Evaluating the model\n",
    "fine_tuned_model = PegasusForConditionalGeneration.from_pretrained(\"./fine_tuned_pegasus_v2\")\n",
    "fine_tuned_tokenizer = PegasusTokenizer.from_pretrained(\"./fine_tuned_pegasus_v2\")\n",
    "\n",
    "def paraphrase(text, model=fine_tuned_model, tokenizer=fine_tuned_tokenizer, max_length=50, num_beams=10, temperature=1.5):\n",
    "    input_text = tokenizer(text, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        **input_text,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        temperature=temperature,\n",
    "        early_stopping=True,\n",
    "        do_sample=True\n",
    "    )\n",
    "    paraphrased_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return paraphrased_text\n",
    "\n",
    "#paraphrasing function interactively\n",
    "while True:\n",
    "    user_input = input(\"Enter a sentence to paraphrase (or 'exit' to quit): \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    paraphrased_output = paraphrase(user_input)\n",
    "    print(\"Paraphrased Output:\", paraphrased_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f33ab67-8284-46de-968b-1d825f14e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the trained model from above\n",
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "# Loading the fine-tuned model and tokenizer\n",
    "fine_tuned_model = PegasusForConditionalGeneration.from_pretrained(\"./fine_tuned_pegasus_v2\")\n",
    "fine_tuned_tokenizer = PegasusTokenizer.from_pretrained(\"./fine_tuned_pegasus_v2\")\n",
    "\n",
    "#paraphrasing function\n",
    "def paraphrase(text, model=fine_tuned_model, tokenizer=fine_tuned_tokenizer, max_length=50, num_beams=10, temperature=1.5):\n",
    "    input_text = tokenizer(text, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        **input_text,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        temperature=temperature,\n",
    "        early_stopping=True,\n",
    "        do_sample=True\n",
    "    )\n",
    "    paraphrased_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return paraphrased_text\n",
    "\n",
    "# Using the paraphrasing function\n",
    "while True:\n",
    "    sentence = input(\"Enter a sentence to paraphrase (or 'exit' to quit): \")\n",
    "    if sentence.lower() == 'exit':\n",
    "        break\n",
    "    paraphrased_output = paraphrase(sentence)\n",
    "    print(\"Paraphrased sentence:\", paraphrased_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
